{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crawl listing information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Import appropriate libraries\n",
    "import pandas as pd\n",
    "import urllib.request #urllib request is a url communication library\n",
    "import time #library to add delay to counter the anti-crawling algorithm\n",
    "from bs4 import BeautifulSoup # beautiful soup library\n",
    "#Data parse of of all the district in shanghai\n",
    "\n",
    "df_neighborhood = pd.read_csv(\"C:/Users/melvi/Desktop/Capstone/Section_Raw.csv\", encoding = \"gbk\")\n",
    "\n",
    "section_list = df_neighborhood[\"Section\"]\n",
    "neighborhood1_list = section_list[1:21]\n",
    "\n",
    "\n",
    "section_in_chinese = df_neighborhood[\"Section in Chinese\"]\n",
    "\n",
    "def page_counter(url):\n",
    "    \n",
    "    page =  BeautifulSoup(url, 'lxml')\n",
    "    listing = BeautifulSoup(url, 'lxml')\n",
    "    listing_number = listing.select(\"#content > div.leftContent > div.resultDes.clear > h2 > span\")\n",
    "    listing_number = int(str(listing_number[0]).split(\">\")[1].split(\"<\")[0])\n",
    "    if listing_number == 0:\n",
    "        page_count = 0\n",
    "    else: \n",
    "        page_count = page.select('#content > div.leftContent > div.contentBottom.clear > div.page-box.fr > div')\n",
    "        page_count = str(page_count[0]).split(\":\")[1].split(\",\")[0]\n",
    "        \n",
    "    return page_count\n",
    "\n",
    "def parse_HTMLdata(htmlstr):\n",
    "    #create a beautifulsoup object, the second argument is the engine.\n",
    "    sp =  BeautifulSoup(htmlstr, 'lxml')\n",
    "    \n",
    "    #get the housing information\n",
    "    \n",
    "    house_list = sp.select('#content > div.leftContent > ul > li')\n",
    "    page_list = []\n",
    "    \n",
    "    district = BeautifulSoup(htmlstr, 'lxml')\n",
    "    district = district.select('body > div:nth-of-type(3) > div > div.position > dl:nth-of-type(2)  > dd > div:nth-of-type(1) > div:nth-of-type(1) > a.selected')\n",
    "    district = str(district[0]).split(\">\")[1].split(\"<\")[0]\n",
    "    \n",
    "\n",
    "    \n",
    "    for house in house_list:\n",
    "    \n",
    "        rows_list = []\n",
    "        \n",
    "        title = house.select('div.info.clear > div.title > a')\n",
    "        title = (title[0].text).strip()\n",
    "        rows_list.append(title)\n",
    "\n",
    "        \n",
    "        infos = house.select('div.info.clear > div.address > div')\n",
    "        infos = (infos[0].text).strip()\n",
    "        layout = infos.split(\"|\")[0]\n",
    "        area = infos.split(\"|\")[1]\n",
    "        orientation = infos.split(\"|\")[2]\n",
    "        decoration = infos.split(\"|\")[3]\n",
    "        floor = infos.split(\"|\")[4]\n",
    "        year = infos.split(\"|\")[5]\n",
    "        house_type = infos.split(\"|\")[6]\n",
    "\n",
    "        rows_list.append(layout)  \n",
    "        rows_list.append(area)\n",
    "        rows_list.append(orientation)\n",
    "        rows_list.append(decoration)\n",
    "        rows_list.append(floor)\n",
    "        rows_list.append(year)\n",
    "        rows_list.append(house_type)\n",
    " \n",
    "\n",
    "        addr_community = house.select('div.info.clear > div.flood > div > a:nth-of-type(1)')\n",
    "        addr_community = (addr_community[0].text).strip()\n",
    "        rows_list.append(addr_community)   \n",
    " \n",
    "        addr_neighborhood = house.select('div.info.clear > div.flood > div > a:nth-of-type(2)')\n",
    "        addr_neighborhood = (addr_neighborhood[0].text).strip()\n",
    "        rows_list.append(addr_neighborhood)   \n",
    "\n",
    "        url_community = house.select('div.info.clear > div.flood > div > a:nth-of-type(1)')\n",
    "        url_community = str(url_community[0]).split(\"/\")[4]\n",
    "        rows_list.append(url_community)    \n",
    "\n",
    "\n",
    "        rows_list.append(district)\n",
    "        \n",
    "        total_price = house.select('div.info.clear > div.priceInfo > div.totalPrice > span')\n",
    "        total_price = (total_price[0].text).strip()\n",
    "        rows_list.append(total_price)   \n",
    "\n",
    "        unit_price = house.select('div.info.clear > div.priceInfo > div.unitPrice > span')\n",
    "        unit_price = (unit_price[0].text).strip()\n",
    "        rows_list.append(unit_price)\n",
    "\n",
    "        \n",
    "        following_info = house.select('div.info.clear > div.followInfo')\n",
    "        following_info = (following_info[0].text).strip()\n",
    "        rows_list.append(following_info)\n",
    "      \n",
    "        \n",
    "        subway_tag = house.select('div.info.clear > div.tag > span.subway')\n",
    "        if subway_tag == []:\n",
    "            subway_tag = 0\n",
    "        else:\n",
    "            subway_tag = 1\n",
    "        rows_list.append(subway_tag)\n",
    "   \n",
    "        good_tag = house.select('div.info.clear > div.tag > span.good')\n",
    "        if good_tag == []:\n",
    "            good_tag = 0\n",
    "        else:\n",
    "            good_tag = 1\n",
    "        rows_list.append(good_tag)\n",
    "    \n",
    "        vr_tag = house.select('div.info.clear > div.tag > span.vr')\n",
    "        if vr_tag == []:\n",
    "            vr_tag = 0\n",
    "        else:\n",
    "            vr_tag = 1\n",
    "        rows_list.append(vr_tag)\n",
    "        \n",
    "        taxfree_tag = house.select('div.info.clear > div.tag > span.taxfree')\n",
    "        if taxfree_tag == []:\n",
    "            taxfree_tag = 0\n",
    "        else:\n",
    "            taxfree_tag = 1\n",
    "        rows_list.append(taxfree_tag)\n",
    "        \n",
    "        haskey_tag = house.select('div.info.clear > div.tag > span.haskey')\n",
    "        if haskey_tag == []:\n",
    "            haskey_tag = 0\n",
    "        else:\n",
    "            haskey_tag = 1\n",
    "        rows_list.append(haskey_tag)\n",
    "        \n",
    "        five_tag = house.select('div.info.clear > div.tag > span.five')\n",
    "        if five_tag == []:\n",
    "            five_tag = 0\n",
    "        else:\n",
    "            five_tag = 1\n",
    "        rows_list.append(five_tag)\n",
    "\n",
    "        \n",
    "        page_list.append(rows_list)\n",
    "    \n",
    "    return page_list\n",
    "\n",
    "\n",
    "#Data crawling of of all the district in shanghai\n",
    "def request_data(url):\n",
    "    \n",
    "    '''crawl current website data,parameter is the website address, return data list'''\n",
    "   \n",
    "    #create request object\n",
    "    req = urllib.request.Request(url)\n",
    "    \n",
    "    #use with as expression to save resouces.\n",
    "    page_data_list = []\n",
    "    \n",
    "    with urllib.request.urlopen(req) as response:\n",
    "        #get the string object\n",
    "        data = response.read()\n",
    "        htmlstr = data.decode()\n",
    "        L = parse_HTMLdata(htmlstr)\n",
    "        #use extend instead of append, as we don't want nesting\n",
    "        page_data_list.extend(L)\n",
    "    \n",
    "    return page_data_list\n",
    "\n",
    "def request_count(url):\n",
    "    req = urllib.request.Request(url)\n",
    "    \n",
    "    with urllib.request.urlopen(req) as response:\n",
    "        #get the string object\n",
    "        data = response.read()\n",
    "        htmlstr = data.decode()\n",
    "        count = page_counter(htmlstr)\n",
    "        #use extend instead of append, as we don't want nesting\n",
    " \n",
    "    return count\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "#Lianjia Shanghai second-hand housing website\n",
    "url_temp = 'https://sh.lianjia.com/ershoufang/{}/pg{}/'\n",
    "\n",
    "#create a data list\n",
    "data_list = []\n",
    "neighborhood1_list = [\"beicai\"]\n",
    "for neighborhood in neighborhood1_list:\n",
    "    count = request_count(url_temp.format(neighborhood,1))\n",
    "    print(count)\n",
    "    i = 1\n",
    "    while i <= int(count):\n",
    "        url = url_temp.format(neighborhood,i)\n",
    "        print(url)\n",
    "        print(\"++++++++++++{} neighborhood:Page{}++++++++++++++\".format(neighborhood,i))\n",
    "        \n",
    "        try :        \n",
    "            # Counter anti-crawling\n",
    "            # sleep 15 seconds\n",
    "            time.sleep(15)\n",
    "            L = request_data(url)\n",
    "            data_list.extend(L)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            time.sleep(20)\n",
    "            try : \n",
    "                L = request_data(url)\n",
    "                data_list.extend(L)\n",
    "                continue\n",
    "            except Exception as e1:\n",
    "                print(e1)\n",
    "                \n",
    "            # stop looping\n",
    "            print('Stop Looping')\n",
    "            break\n",
    "        i = i + 1\n",
    "print('data_list2 =', len(data_list))\n",
    "print('Finish')\n",
    "\n",
    "#Save data\n",
    "colsname = ['Title', 'layout','area','orientation','decoration','floor','year','house_type', 'Community', 'Neighborhood','Community URL','District', 'Total Price', 'Unit Price', 'Follow Infos','Subway','Good','VR','Five','Has Key','Two']   \n",
    "\n",
    "df = pd.DataFrame(data_list, columns=colsname)    \n",
    "df.to_csv('C:/Users/melvi/Desktop/Capstone/house_data4.csv', index=False, encoding='gbk')\n",
    "\n",
    "print('Data Saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Community info crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import urllib.request #urllib request is a url communication library\n",
    "import time #library to add delay to counter the anti-crawling algorithm\n",
    "from bs4 import BeautifulSoup # beautiful soup library\n",
    "#Data parse of of all the district in shanghai\n",
    "\n",
    "input_df = pd.read_excel('C:/Users/melvi/Desktop/Capstone/Missing Community.xlsx', encoding='gbk')\n",
    "\n",
    "community_list = (input_df[\"ID\"].unique())\n",
    "\n",
    "community_list = pd.Series(community_list)\n",
    "\n",
    "community_list = community_list.drop_duplicates()\n",
    "\n",
    "community_list_int = []\n",
    "\n",
    "for num in community_list:\n",
    "    community_list_int.append(int(num))\n",
    "    \n",
    "    \n",
    "\n",
    "#community_list.to_csv('C:/Users/melvi/Desktop/Capstone/community_list.csv', index=False, encoding='gbk')\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Sat Nov  9 19:05:22 2019\n",
    "\n",
    "@author: melvi\n",
    "\"\"\"\n",
    "def parse_HTMLdata(htmlstr):\n",
    "    #create a beautifulsoup object, the second argument is the engine.\n",
    "    sp =  BeautifulSoup(htmlstr, 'lxml')\n",
    "    \n",
    "    #get the housing information\n",
    "    \n",
    "    house_list = sp.select('body')\n",
    "    page_list = []\n",
    "\n",
    "\n",
    "    \n",
    "    for house in house_list:\n",
    "    \n",
    "        rows_list = []\n",
    "        \n",
    "        Community_Name = house.select('div.xiaoquDetailHeader > div > div.detailHeader.fl > h1')\n",
    "        Community_Name = str(Community_Name[0]).split(\">\")[1].split(\"<\")[0]\n",
    "        rows_list.append(Community_Name)\n",
    "        \n",
    "        Community_Address = house.select('div.xiaoquDetailHeader > div > div.detailHeader.fl > div')\n",
    "        Community_Address = str(Community_Address[0]).split(\">\")[1].split(\"<\")[0]\n",
    "        rows_list.append(Community_Address)\n",
    "        \n",
    "        rows_list.append(Community)\n",
    "        \n",
    "        Community_Price = house.select('div.xiaoquOverview > div.xiaoquDescribe.fr > div.xiaoquPrice.clear > div > span.xiaoquUnitPrice')\n",
    "        Community_Price = (Community_Price[0].text).strip()\n",
    "        rows_list.append(Community_Price)\n",
    "        \n",
    "        \n",
    "        Build_Year = house.select('div.xiaoquOverview > div.xiaoquDescribe.fr > div.xiaoquInfo > div:nth-of-type(1) > span.xiaoquInfoContent')\n",
    "        Build_Year = (Build_Year[0].text).strip()\n",
    "        rows_list.append(Build_Year)\n",
    "        \n",
    "        Buiding_Type = house.select('div.xiaoquOverview > div.xiaoquDescribe.fr > div.xiaoquInfo > div:nth-of-type(2) > span.xiaoquInfoContent')\n",
    "        Buiding_Type = (Buiding_Type[0].text).strip()\n",
    "        rows_list.append(Buiding_Type)\n",
    "        \n",
    "        Maintenance_Cost= house.select('div.xiaoquOverview > div.xiaoquDescribe.fr > div.xiaoquInfo > div:nth-of-type(3) > span.xiaoquInfoContent')\n",
    "        Maintenance_Cost = (Maintenance_Cost[0].text).strip()\n",
    "        rows_list.append(Maintenance_Cost)\n",
    "        \n",
    "        Company = house.select('div.xiaoquOverview > div.xiaoquDescribe.fr > div.xiaoquInfo > div:nth-of-type(4) > span.xiaoquInfoContent')\n",
    "        Company = (Company[0].text).strip()\n",
    "        rows_list.append(Company)\n",
    "        \n",
    "        Developer = house.select('div.xiaoquOverview > div.xiaoquDescribe.fr > div.xiaoquInfo > div:nth-of-type(5) > span.xiaoquInfoContent')\n",
    "        Developer = (Developer[0].text).strip()\n",
    "        rows_list.append(Developer)\n",
    "        \n",
    "        Total_Building = house.select('div.xiaoquOverview > div.xiaoquDescribe.fr > div.xiaoquInfo > div:nth-of-type(6) > span.xiaoquInfoContent')\n",
    "        Total_Building = (Total_Building[0].text).strip()\n",
    "        rows_list.append(Total_Building)\n",
    "        \n",
    "        Total_House = house.select('div.xiaoquOverview > div.xiaoquDescribe.fr > div.xiaoquInfo > div:nth-of-type(7) > span.xiaoquInfoContent')\n",
    "        Total_House = (Total_House[0].text).strip()\n",
    "        rows_list.append(Total_House)\n",
    "\n",
    "        page_list.append(rows_list)\n",
    "    \n",
    "    return page_list\n",
    "\n",
    "\n",
    "#Data crawling of of all the district in shanghai\n",
    "def request_data(url):\n",
    "    \n",
    "    '''crawl current website data,parameter is the website address, return data list'''\n",
    "   \n",
    "    #create request object\n",
    "    req = urllib.request.Request(url)\n",
    "    \n",
    "    #use with as expression to save resouces.\n",
    "    page_data_list = []\n",
    "    \n",
    "    with urllib.request.urlopen(req) as response:\n",
    "        #get the string object\n",
    "        data = response.read()\n",
    "        htmlstr = data.decode()\n",
    "        L = parse_HTMLdata(htmlstr)\n",
    "        #use extend instead of append, as we don't want nesting\n",
    "        page_data_list.extend(L)\n",
    "    \n",
    "    return page_data_list\n",
    "\n",
    "\n",
    "\n",
    "#Lianjia Shanghai second-hand housing website\n",
    "url_temp = 'https://sh.lianjia.com/xiaoqu/{}/'\n",
    "\n",
    "#create a data list\n",
    "data_list = []\n",
    "i = 0\n",
    "sleep_time = 2\n",
    "for Community in community_list:\n",
    "    url = url_temp.format(Community)\n",
    "    print(url)\n",
    "    print(\"++++++++++++{} Community++++++++++++++\".format(Community))\n",
    "        \n",
    "    try:        \n",
    "        # Counter anti-crawling\n",
    "        # sleep 10 seconds\n",
    "        time.sleep(sleep_time)\n",
    "        L = request_data(url)\n",
    "        data_list.extend(L)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        time.sleep(sleep_time)\n",
    "        Community = Community + 1\n",
    "        try: \n",
    "            L = request_data('https://sh.lianjia.com/xiaoqu/{}/'.format(Community))\n",
    "            data_list.extend(L)\n",
    "        except Exception as e10:\n",
    "            print(e)\n",
    "            time.sleep(sleep_time)\n",
    "            Community = Community + 1\n",
    "            try: \n",
    "                L = []\n",
    "                data_list.extend(L)\n",
    "                continue\n",
    "            except Exception as e10:\n",
    "                print(e10)\n",
    "                print('Stop Looping')\n",
    "                break\n",
    "    print(i)\n",
    "    i = i + 1\n",
    "\n",
    "print('data_list2 =', len(data_list))\n",
    "print('Finish')\n",
    "#Save data\n",
    "colsname = ['Community Name','Community Address','Community','Community Price','Build Year','Buiding_Type','Maintenance_Cost','Company','Developer','Total_Building','Total_House']   \n",
    "        \n",
    "\n",
    "df = pd.DataFrame(data_list, columns=colsname)    \n",
    "df.to_csv('C:/Users/melvi/Desktop/Capstone/Filled Community.csv', index=False, encoding='gbk')\n",
    "\n",
    "print('Data Saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connect to BaiduMap API to obtain lattitude and longitude of each community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "from urllib.request import urlopen,quote\n",
    "\n",
    "geocoder = pd.DataFrame(columns=['Community','Latitude','Longitude'])\n",
    "latitude = []\n",
    "longitude = []\n",
    "community_list = []\n",
    "def getlnglat(address):\n",
    "    url = 'http://api.map.baidu.com/geocoding/v3/?address='\n",
    "    output = 'json'\n",
    "    ak = 'myak'\n",
    "    add = quote(address)\n",
    "    url2 = url+add+'&output='+output+\"&ak=\" +ak\n",
    "    req = urlopen(url2)\n",
    "    res  = req.read().decode()\n",
    "    temp = json.loads(res)\n",
    "    lng = temp['result']['location']['lng']\n",
    "    lat = temp['result']['location']['lat']\n",
    "\n",
    "    community_list.append(community)\n",
    "    latitude.append(lat)\n",
    "    longitude.append(lng)\n",
    "\n",
    "data_1 = pd.read_excel(\"C:/Users/melvi/Desktop/Capstone/Community info.xlsx\", encoding = \"gbk\")\n",
    "i = 0\n",
    "for community in data_1[\"Community\"]:\n",
    "    community=community.strip()\n",
    "    getlnglat(community)\n",
    "    i = i + 1\n",
    "    print(i)\n",
    "    \n",
    "geocoder[\"Community\"] = community_list\n",
    "geocoder[\"Latitude\"] = latitude\n",
    "geocoder[\"Longitude\"] = longitude\n",
    "    \n",
    "\n",
    "geocoder.to_csv(\"C:/Users/melvi/Desktop/Capstone/geocode.csv\",encoding = \"gbk\")    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get facility information within 1km raidius"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from urllib.request import urlopen,quote\n",
    "import math\n",
    "import time\n",
    "\n",
    "place = pd.DataFrame(columns=['Community','Page','Name','Distance','Tag'])\n",
    "name_list = []\n",
    "community_list = []\n",
    "distance_list = []\n",
    "tag_list = []\n",
    "page_list = []\n",
    "\n",
    "data_1 = pd.read_csv(\"C:/Users/melvi/Desktop/Capstone/geocode.csv\", encoding = \"gbk\")\n",
    "n=1020\n",
    "m=2\n",
    "place_list = data_1[n:]\n",
    "i = n\n",
    "for community in place_list['Community']:\n",
    "    print(community)\n",
    "    geocode = \"{},{}\".format(place_list[\"Latitude\"][i],place_list[\"Longitude\"][i])\n",
    "    time.sleep(0)\n",
    "    count=0\n",
    "    url = 'http://api.map.baidu.com/place/v2/search?query='\n",
    "    location= geocode\n",
    "    radius = str(1000)\n",
    "    output = 'json'\n",
    "    ak = 'myak'\n",
    "    query = quote(\"购物中心$公园$中学$小学$综合医院$专科医院$地铁站\")\n",
    "    #'&tag=' +tag+\n",
    "    url2 = url+query+ '&location='+ location + \"&radius=\"+radius+ \"&output=\"+output+\"&scope=2\"+'&page_size=20'+\"&ak=\"+ak\n",
    "    req = urlopen(url2)\n",
    "    res  = req.read().decode()\n",
    "    temp = json.loads(res)\n",
    "    print(temp)\n",
    "    page_num = math.ceil(int(temp['total'])/20) - 1\n",
    "    page_count = 0\n",
    "    try:\n",
    "        count_limit = len(temp['results'])\n",
    "        count=0\n",
    "        while count < count_limit:\n",
    "            name = temp['results'][count]['name']\n",
    "            distance = temp['results'][count]['detail_info']['distance']\n",
    "            tag = temp['results'][count]['detail_info']['tag']\n",
    "            count = count + 1\n",
    "            name_list.append(name)\n",
    "            distance_list.append(distance)\n",
    "            tag_list.append(tag)\n",
    "            community_list.append(community)\n",
    "            page_list.append(page_count)\n",
    "\n",
    "        \n",
    "        page_count = 1\n",
    "        while page_count <= page_num:\n",
    "            page_count_str = str(page_count)\n",
    "            count=0\n",
    "            url3 = url+query+ '&location='+ location + \"&radius=\"+radius+ \"&output=\"+output+\"&scope=2\"+'&page_size=20'+'&page_num='+page_count_str+\"&ak=\"+ak\n",
    "            req = urlopen(url3)\n",
    "            res  = req.read().decode()\n",
    "            temp = json.loads(res)\n",
    "\n",
    "            count_limit = len(temp['results'])\n",
    "            count=0\n",
    "            while count < count_limit:\n",
    "                name = temp['results'][count]['name']\n",
    "                distance = temp['results'][count]['detail_info']['distance']\n",
    "                tag = temp['results'][count]['detail_info']['tag']\n",
    "                count = count + 1\n",
    "                name_list.append(name)\n",
    "                distance_list.append(distance)\n",
    "                tag_list.append(tag)\n",
    "                community_list.append(community)\n",
    "                page_list.append(page_count)\n",
    "            page_count = page_count + 1\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        try : \n",
    "            count_limit = len(temp['results'])\n",
    "            count=0\n",
    "            while count < count_limit:\n",
    "                print(count)\n",
    "                name = temp['results'][count]['name']\n",
    "                distance = temp['results'][count]['detail_info']['distance']\n",
    "                tag = temp['results'][count]['detail_info']['tag']\n",
    "                count = count + 1\n",
    "                name_list.append(name)\n",
    "                distance_list.append(distance)\n",
    "                tag_list.append(tag)\n",
    "                community_list.append(community)\n",
    "                page_list.append(page_count)\n",
    "    \n",
    "            \n",
    "            page_count = 1\n",
    "            while page_count <= page_num:\n",
    "                page_count_str = str(page_count)\n",
    "                url3 = url+query+ '&location='+ location + \"&radius=\"+radius+ \"&output=\"+output+\"&scope=2\"+'&page_size=20'+'&page_num='+page_count_str+\"&ak=\"+ak\n",
    "                req = urlopen(url3)\n",
    "                res  = req.read().decode()\n",
    "                temp = json.loads(res)\n",
    "                count_limit = len(temp['results'])\n",
    "                count=0\n",
    "                while count < count_limit:\n",
    "                    name = temp['results'][count]['name']\n",
    "                    distance = temp['results'][count]['detail_info']['distance']\n",
    "                    tag = temp['results'][count]['detail_info']['tag']\n",
    "                    count = count + 1\n",
    "                    name_list.append(name)\n",
    "                    distance_list.append(distance)\n",
    "                    tag_list.append(tag)\n",
    "                    community_list.append(community)\n",
    "                    page_list.append(page_count)\n",
    "                page_count = page_count + 1\n",
    "                \n",
    "        except Exception as e1:\n",
    "            print(e1)\n",
    "            try : \n",
    "                count_limit = len(temp['results'])\n",
    "                count=0\n",
    "                while count < count_limit:\n",
    "                    print(count)\n",
    "                    name = temp['results'][count]['name']\n",
    "                    distance = temp['results'][count]['detail_info']['distance']\n",
    "                    tag = []\n",
    "                    count = count + 1\n",
    "                    name_list.append(name)\n",
    "                    distance_list.append(distance)\n",
    "                    tag_list.append(tag)\n",
    "                    community_list.append(community)\n",
    "                    page_list.append(page_count)\n",
    "    \n",
    "            \n",
    "                page_count = 1\n",
    "                while page_count <= page_num:\n",
    "                    page_count_str = str(page_count)\n",
    "                    url3 = url+query+ '&location='+ location + \"&radius=\"+radius+ \"&output=\"+output+\"&scope=2\"+'&page_size=20'+'&page_num='+page_count_str+\"&ak=\"+ak\n",
    "                    req = urlopen(url3)\n",
    "                    res  = req.read().decode()\n",
    "                    temp = json.loads(res)\n",
    "                    count_limit = len(temp['results'])\n",
    "                    count=0\n",
    "                    while count < count_limit:\n",
    "                        name = temp['results'][count]['name']\n",
    "                        distance = temp['results'][count]['detail_info']['distance']\n",
    "                        tag = []\n",
    "                        count = count + 1\n",
    "                        name_list.append(name)\n",
    "                        distance_list.append(distance)\n",
    "                        tag_list.append(tag)\n",
    "                        community_list.append(community)\n",
    "                        page_list.append(page_count)\n",
    "                    page_count = page_count + 1                \n",
    "                    continue\n",
    "                    \n",
    "            except Exception as e2:\n",
    "                print(e2)\n",
    "          \n",
    "                print('Stop Looping') \n",
    "                break    \n",
    "        \n",
    "\n",
    "    print(i)\n",
    "    i = i + 1\n",
    "\n",
    "\n",
    "place[\"Name\"] = name_list\n",
    "place[\"Community\"] = community_list\n",
    "place[\"Distance\"] = distance_list\n",
    "place[\"Tag\"] = tag_list\n",
    "place[\"Page\"] = page_list\n",
    "\n",
    "place.to_csv(\"C:/Users/melvi/Desktop/Capstone/place.csv\".format(n),encoding = \"gbk\")    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get the distance from the centre and nearest top shopping malls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "df1 = pd.read_csv(\"C:/Users/melvi/Desktop/Capstone/final_1.csv\",encoding = \"gbk\")\n",
    "\n",
    "\n",
    "#calculate distance to the centre of Shanghai\n",
    "EARTH_REDIUS = 6378.137\n",
    "\n",
    "df_geo = pd.read_excel(\"C:/Users/melvi/Desktop/Capstone/geocoder.xlsx\",encoding = \"gbk\")\n",
    "\n",
    "def rad(d):\n",
    "    return d * np.pi/ 180.0\n",
    "\n",
    "def getDistance(lat1, lng1, lat2, lng2):\n",
    "    radLat1 = rad(lat1)\n",
    "    radLat2 = rad(lat2)\n",
    "    a = radLat1 - radLat2\n",
    "    b = rad(lng1) - rad(lng2)\n",
    "    s = 2 * math.asin(math.sqrt(math.pow(np.sin(a/2), 2) + np.cos(radLat1) * np.cos(radLat2) * math.pow(np.sin(b/2), 2)))\n",
    "    s = s * EARTH_REDIUS\n",
    "    return s\n",
    "\n",
    "centre =[31.236968,121.482341]\n",
    "wu= [31.305458,121.522215]\n",
    "xu = [31.201202,121.442314]\n",
    "dong = [31.243738,121.490857]\n",
    "xi = [31.236045,121.466545]\n",
    "xin = [31.221998,121.481686]\n",
    "lu = [31.243812,121.508984]\n",
    "huan = [31.239028,121.419181]\n",
    "zhong = [31.224734,121.425104]\n",
    "\n",
    "i = 0\n",
    "df_dist_centre = []\n",
    "community_list = []\n",
    "df_dist_shop = []\n",
    "\n",
    "while i <8677:\n",
    "    community = df_geo[\"Community\"][i]\n",
    "    community_list.append(community)\n",
    "    lat1 = df_geo[\"Latitude\"][i]\n",
    "    lng1 = df_geo[\"Longitude\"][i]\n",
    "    lat2 = centre[0]\n",
    "    lng2 = centre[1]\n",
    "    dist_centre = getDistance(lat1, lng1, lat2, lng2)\n",
    "    lat3 = wu[0]\n",
    "    lng3 = wu[1]    \n",
    "    lat4 = xu[0]\n",
    "    lng4 = xu[1] \n",
    "    lat5 = dong[0]\n",
    "    lng5 = dong[1] \n",
    "    lat6 = xin[0]\n",
    "    lng6 = xin[1] \n",
    "    lat7 = lu[0]\n",
    "    lng7 = lu[1] \n",
    "    lat8 = huan[0]\n",
    "    lng8 = huan[1] \n",
    "    lat9 = zhong[0]\n",
    "    lng9 = zhong[1]\n",
    "    dist_1 = getDistance(lat1, lng1, lat3, lng3)\n",
    "    dist_2 = getDistance(lat1, lng1, lat4, lng4)\n",
    "    dist_3 = getDistance(lat1, lng1, lat5, lng5)\n",
    "    dist_4 = getDistance(lat1, lng1, lat6, lng6)\n",
    "    dist_5 = getDistance(lat1, lng1, lat7, lng7)\n",
    "    dist_6 = getDistance(lat1, lng1, lat8, lng8)\n",
    "    dist_7 = getDistance(lat1, lng1, lat9, lng9)\n",
    "    dist_shop = min(dist_1,dist_2,dist_3,dist_4,dist_5,dist_6,dist_7)\n",
    "    df_dist_shop.append(dist_shop)\n",
    "\n",
    "    df_dist_centre.append(dist_centre)\n",
    "    i = i+1\n",
    "    print(i)\n",
    "   \n",
    "Distance = pd.DataFrame(columns=['Community','Distance centre',\"Distance shop\"]) \n",
    "\n",
    "Distance[\"Community\"] = community_list\n",
    "Distance[\"Distance centre\"] = df_dist_centre\n",
    "Distance[\"Distance shop\"] = df_dist_shop\n",
    "\n",
    "  \n",
    "\n",
    "Distance.to_csv(\"C:/Users/melvi/Desktop/Capstone/distance.csv\",index = False, encoding = \"gbk\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
